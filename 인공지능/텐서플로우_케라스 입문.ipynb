{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의류 데이터 분류하는 모델 만들어보기 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf # 텐서플로우 import \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "source": [
    "# 데이터 불러오기 & 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() # 데이터 불러오기. 튜플로 묶어서 반환"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 세트와 검증 세트로 분류. train_test_split() 사용\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify = y_train_all, test_size = 0.2, random_state = 42) # random_state는 안해도 되는데 예제랑 같다는 걸 보여주기 위해 사용한 것. 20%(test_size = 0.2)를 검증 세트로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 정규화. 0~1사이 값으로 나타내기 위해 255로 나눠줌 \n",
    "x_train = x_train / 255\n",
    "x_val = x_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28*28를 784(28*28하면 나오는거)로 변경. 왜냐하면 내가 설계한 모델은 1차원 배열을 요구하기 때문.\n",
    "#한 줄로 쫙 이어붙혔다고 보면 된다.\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_val = x_val.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 데이터들을 원-핫 인코딩 처리\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "source": [
    "### 텐서플로우로 모델 생성(without keras)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [48000] vs. [37632000] [Op:Sub]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5a97dcf773f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mz_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m \u001b[0;31m# 출력값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mz_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msqr_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz_net\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#오차 제곱을 손실 함수로 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmean_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 손실 함수에 대한 가중치의 그래디언트 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10305\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10306\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10307\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10308\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10309\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [48000] vs. [37632000] [Op:Sub]"
     ]
    }
   ],
   "source": [
    "# 가중치, 절편 초기화\n",
    "w = tf.Variable(tf.zeros(shape =(1)))\n",
    "b = tf.Variable(tf.zeros(shape =(1)))\n",
    "\n",
    "# 경사 하강법 옵티마이저 설정(=학습법 설정)\n",
    "optimizer = tf.optimizers.SGD(lr = 0.01) # 학습률 0.01의 확률적 경사 하강법(하나씩 훈련)으로 훈련시킬 것\n",
    "#에포크 횟수 설정\n",
    "num_epochs = 10\n",
    "for step in range(num_epochs):\n",
    "    #자동 미분을 위해 연산 과정 기록\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_net = w * x_train + b # 출력값\n",
    "        z_net = tf.reshape(z_net, [-1]) \n",
    "        sqr_errors = tf.square(y_train - z_net) #오차 제곱을 손실 함수로 사용\n",
    "        mean_cost = tf.reduce_mean(sqr_errors)\n",
    "    # 손실 함수에 대한 가중치의 그래디언트 계산\n",
    "    grads = tape.gradient(mean_cost, [w,b])\n",
    "    # 옵티마이저에 그레디언트 반영(가중치, 절편 값 수정. -> 학습)\n",
    "    optimizer.apply_gradients(zip(grads, [w,b]))\n",
    "\n",
    "# 에러 뜬다. 왜지? 하씨벌...모르겠다. "
   ]
  },
  {
   "source": [
    "### keras를 이용 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 1s 439us/step - loss: nan\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 1s 416us/step - loss: nan\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 1s 441us/step - loss: nan\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 1s 447us/step - loss: nan\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 1s 423us/step - loss: nan\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 1s 466us/step - loss: nan\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 1s 397us/step - loss: nan\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 1s 384us/step - loss: nan\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 1s 484us/step - loss: nan\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 1s 449us/step - loss: nan\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8482102b80>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = tf.keras.models.Sequential() # Sequential 클래스로 모델 생성. Sequential 클래스는 '순차적으로 레이어를 쌓은 신경망 모델\n",
    "# 완전 연결층(Dense Layer) 추가\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "# 옵티마이저(학습법), 손실 함수 지정\n",
    "model.compile(optimizer = 'sgd', loss = 'mse')\n",
    "# 훈련 데이터를 사용하여 에포크 횟수만큼 훈련\n",
    "model.fit(x_train, y_train, epochs = 10)\n",
    "\n",
    "# 코드 양이 확 줄었음.\n",
    "# 케라스는 다른 라이브러리에서도 사용 가능. 텐서플로우는 케라스를 주력 파이썬 API로 지정함. tensorflow.keras는 케라스 명세를 따르는 텐서플로우만을 위한 구현임.\n",
    "# 위 코드는 은닉층, 출력층을 Dense 클래스의 객체(Dense Layer)로 구성하고 이를 Sequential 객체(모델)에 추가한 것.\n",
    "# 마치 층을 쌓으며 모델을 만드는 것 같아 직관적으로 인공신경망의 층을 설계할 수 있다고 말함. 앞서 한 클래스에 가중치, 절편을 다 때려넣은 MultiClassNetwork와 비교해보면 정말 직관적이라는 사실을 알 수 있음."
   ]
  },
  {
   "source": [
    "## Sequential 클래스 사용 방법"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 연결 신경망을 만들려면 Sequential와 Dense를 함께 사용해야함. Sequential를 초기화할 때 층을 추가찰 수도 있고 빈 공간으로 초기화 후 add()로 층을 추가하는 방법이 있음.\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# model = Sequential([Dense(...), ...]) # 방법 1. 초기화할 때 Dense Layer를 추가\n",
    "\n",
    "# 빈 모델로 초기화 후 add로 Layer추가\n",
    "# model.Sequential()\n",
    "# dense = Dense(...)"
   ]
  },
  {
   "source": [
    "## Dense 클래스의 사용 방법"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit : 뉴런의 개수\n",
    "# activation : 활성화 함수\n",
    "# Dense(unit = 100, activation = 'sigmoid') <- 작성 예. 활성화 함수는 softmax, tanh, relu 등 종류가 많다. \n",
    "# keras에서는 가중치를 커널(kernel)이라고 부른다.(커널이랑 가중치랑 의미가 같으니 혼동하지 않기) 가중치를 규제하기 위해 kernel_initializer 매개변수가 존재한다. "
   ]
  },
  {
   "source": [
    "## 모델의 최적화 알고리즘과 손실 함수를 지정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 훈련하기 위해서는 최적화 알고리즘이나 손실 함수 지정\n",
    "# 최적화 알고리즘 : 경사 하강법 알고리즘 \n",
    "# 손실 함수 : 크로스 엔트로피 손실 함수\n",
    "\n",
    "# optimizer : 최적화 알고리즘 지정. compile() 함수에서 최적화 알고리즘과 손실 함수를 지정한다. \n",
    "# loss : 손실 함수. 제곱 오차(정답값과 예측값 차이(=오차값)을 제곱한거는 mse, 로지스틱 손실 함수는 binary_crossentropy, 다중 분류 신경망인 경우에는 cartegorical_crossentropy다.\n",
    "# model.compile(optimizer = 'sgd', loss = 'cartegorical_crossentropy') <- 기본 경사 하강법으로 'sgd' 사용. sgd = 확률적 경사 하강법(데이터 하나씩 학습). 손실 함수는 다중 분류 모델을 사용한다고 하면 cartegorical_crossentropy.\n",
    "# 학습률의 기본값은 0.01"
   ]
  },
  {
   "source": [
    "## 모델의 훈련과 예측"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 클래스의 fit()로 훈련, predict()로 예측.\n",
    "# 검증은 evaluate()를 사용.\n",
    "\n",
    "# 전형적인 Sequential 클래스의 사용 방법(데이터는 전처리 했다고 가정 - 당연히 사용 용도에 맞게 전처리 해놔야함)\n",
    "\n",
    "model = Sequential() # 모델 초기화\n",
    "# 레이어 추가\n",
    "model.add(Dense(...)) # Dense Layer를 추가하는데 이 때 따로 Dense Layer를 선언하기 귀찮으니 add 함수 안에서 Dense Layer를 바로 만들어서 넣어줌. \n",
    "model.add(Dense(...)) \n",
    "# 최적화 알고리즘, 손실 함수 지정\n",
    "model.compile(optimizer='...', loss='...')\n",
    "# 훈련, 예측, 평가\n",
    "model.fit(x, y, epochs = ...) # 훈련\n",
    "model.predict(x) # 예측\n",
    "model.evaluate(x, y) # 평가\n"
   ]
  }
 ]
}