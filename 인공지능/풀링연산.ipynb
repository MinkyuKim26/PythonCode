{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 풀링 연산\n",
    "### 합성곱 신경망에서 합성곱 연산이 일어나는 레이어를 합성곱 레이어(합성곱층), 풀링 연산이 일어나는 레이어를 풀링 레이어(풀링층)라고 부른다. \n",
    "### 합성곱이나 풀링 연산을 거친 결과를 특성 맵(feature map)이라고 부른다. 합성곱 신경망은 일반적으로 합성곱 레이어 다음에 풀링 레이어가 있어 합성곱 레이어의 특성 맵을 풀링 레이어가 입력값으로 받고 이를 다시 특성 맵으로 뽑아내는 구조라고 보면 된다.\n",
    "### 풀링은 입력값을 스캔한 뒤 최대값을 고르거나(최대 플링) 평균값을 계산하는 것(평균 플링)을 말한다. \n",
    "### 풀링 영역의 크기는 보통 2*2를 지정하며 스트라이드(연산을 할 때 미끄러지는 단위) 단위는 풀링 영역의 모서리 크기로 지정한다. 풀링 영역이 곂치지 않게끔 풀링 연산을 수행한다는 뜻.\n",
    "### 2*2 풀링은 입력값의 크기(특성 맵의 크기)를 절반으로 줄인다.(가로 50%, 세로 50%) 즉, 면적은 4배로 줄어든다. 그러면 작은 크기의 특성 맵에서 전체적인 특성을 알 수 있다는 이점이 있다. \n",
    "### 물론 합성곱 연산을 할 때 세임 패딩을 쓰지 않고 스트라이드 크기를 크게 지정해 출력 값(특성 맵)의 크기를 줄일 수 있긴 하다. 그러나 그것 보다는 세임 패딩을 적용해 입력값과 크기가 같은 특성 맵을 뽑아낸 뒤 풀링 연산을 통해 크기가 줄어든 특성맵을 뽑아내는 것이 더 효과적이다. \n",
    "### 연구를 하는 사람들은 최대 플링을 선호한다. 평균 플링은 합성곱 연산을 거친 특성 맵의 특징을 희석시킬 가능성이 높기 때문이다. 합성곱 신경망을 쓰는 이유 중 하나가 이미지 분류 때문에 쓰는데 평균 플링을 사용해 특성을 희석시키면 이미지 분류를 효과적으로 할 수 없게 된다. \n",
    "### 허나 최대 풀링을 할 경우 이미지 특징들이 더 잘 드러나게 되며 이는 이미지 분류에 도움이 된다. 그러므로 평균 플링 대신 최대 플링을 사용하는 것이다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 풀링 수행\n",
    "## 최대 풀링 연산은 max_pool2d()를 사용하면 된다. 매개변수 값으로 풀링 크기와 스트라이드만 전달하면 자동으로 최대 풀링을 수행하여 입력값을 반으로 줄여준다. \n",
    "x = np.array([[1,2,3,4], \n",
    "              [5,6,7,8], \n",
    "              [9,10,11,12],\n",
    "              [13,14,15,16]])\n",
    "x = x.reshape(1,4,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 6,  8],\n",
       "       [14, 16]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# max_pool2d() 사용\n",
    "p_out = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID') # ksize는 풀링의 크기, strides는 스트라이드 크기(풀링 크기와 동일한 값으로 설정). \n",
    "# 반환값은 tensor 객체이며 이를 2*2배열로 변환한다.\n",
    "p_out.numpy().reshape(2,2)"
   ]
  },
  {
   "source": [
    "### 풀링층에는 학습되는 가중치가 없다. 그리고 배치 차원이나 채널 차원으로 적용되지 않기 때문에 풀링층을 통과하기 전후로 배치 크기와 채널 크기는 동일하다. \n",
    "### 풀링 연산은 각 샘플 혹은 채널마다 독립적으로 수행된다. \n",
    "### 합성곱 신경망에서 자주 사용하는 활성화 함수는 렐루(ReLU)다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 렐루(ReLU)\n",
    "### 합성곱 레이어에 사용되는 활성화 함수. 합성곱 신경망의 성능을 더 높혀준다. \n",
    "### 0보다 큰 값은 그대로(y=x) 통과시키고 0보다 작은 값은 0으로 만든다. 다이오드를 생각나게 한다.\n",
    "### 렐루는 넘파이의 maximum()를 사용해 간단하게 구현할 수 있다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0) # 0보다 작으면 0이 더 크므로 0 반환, 0보다 크면 x 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 2, 0, 4, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "x = np.array([-1,2,-3,4,-5])\n",
    "relu(x) # array([0, 2, 0, 4, 0]) 출력\n",
    "\n",
    "# 렐루 함수의 도함수는 0보다 크면 1, 0보다 작으면 0이다. \n",
    "# x=0에서 기울기가 연속이 아니기 때문에 원래라면 0에서 도함수를 정의할 수 없지만 x=0에서 기울기를 0으로 정의해도 잘 돌아가기 때문에 x=0에서 기울기가 0이라고 놓고 쓴다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}