{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 순차 데이터(sequential data)\n",
    "### 이전까지 다뤘던 데이터들은 다른 데이터들과 어떠한 연관이 없는 독립적 객체들이 모인 데이터였다. 그래서 막 섞어가며 훈련 시켜도 아무런 지장이 없었다. \n",
    "### 허나 순차 데이터는 이전 데이터와 연관이 있는 데이터들이다. 예를들면 시간당 기온.(2시 20도, 3시 23도, 4시 22도,...)\n",
    "### 참고로 일정 시간 간격으로 배치된 순차 데이터를 시계열 데이터(time series data)라고 한다. \n",
    "### 대표적인 순차 데이터는 텍스트다. 글자 혹은 단어들의 순서가 맞아야 말이 되는 단어 또는 문장이 만들어지기 때문이다. \n",
    "### 모델에서 순차 데이터를 처리하는 각 단계를 타임 스탭(time step)이라고 부른다. \n",
    "### 예를 들어 'Hello Deep Learning'이라는 문자열이 있을 때 데이터 처리 단위가 단어면 문자열의 타임 스탭은 3이다. 만약 글자가 처리 단위면 19글자가 있으니 타임 스탭은 19가 된다. \n",
    "### 앞서 우리가 배운 완전 연결 신경망으로는 순차 데이터를 처리할 수가 없다. 왜냐하면 완전 연결 신경망은 이전에 입력한 데이터 정보를 저장하는 변수가 없기 때문. \n",
    "### 순차 데이터를 처리하려면 순환 신경망을 이용해야한다. 순환 신경망은 뉴런의 출력이 순환되는 신경망을 말한다. 순환 신경망은 은닉층의 출력이 다시 입력으로 사용된다는 특징이 있다. 이를 순환 구조라고 부르며 순환 구조가 있는 층(레이어)를 순환층이라 부른다. \n",
    "### 은닉층의 출력을 다시 입력으로 사용한다는 말은 앞서 말했듯 이전 입력값의 정보를 이용해 현재 입력값을 처리할 수 있다는 걸 뜻한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 순차 신경망은 레이어나 뉴런을 셀(cell)이라고 부른다. 셀에 있는 모든 뉴런의 순환구조를 표현하는 대신 큰 뉴런 하나에 순환구조를 표현하는 방식을 선택하였다. \n",
    "### 순환 신경망에서는 셀의 출력을 은닉 상태(hidden state)라고 부른다. \n",
    "### 앞서 말했듯 셀에서 정방향 계산을 할 때 이전 출력값이 입력값으로 들어간다. 순환 신경망은 활성화 함수로 하이퍼볼릭 탄젠트(tanh)를 쓰므로 이전 출력값을 H_p라는 행렬로 표시하고 현재 출력값을 H라고 하면 H = tanh(X*W_x + H_p * W+_H + b)가 되는 것이다. \n",
    "###  그러면 Hello Deep Learning에서 Deep에 대한 연산을 할 때 입력값으로 Hello를 넣을 때 나온 출력값을 입력으로 쓰는건가? 그게 맞겠지? \n",
    "### 출력층은 뭐 당연히 분류할 때 주로 쓰는 완전 연결 레이어(Dense Layer)를 주로 쓴다. 딴거 쓸 수도 있는데 분류하려면 Dense Layer만 쓰더라. 다른거 쓰는걸 본 적이 없다.(Z_2 = H*W_2 + b_2, A_2 = sigmoid(Z_2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 순환 레이어의 입력 데이터 크기는 (m, n_f)다. m은 샘플 개수, n_f는 특성 개수다.\n",
    "### 순환 레이어의 가중치 W_1x의 크기는 (n_f, n_c)다. n_f는 특성 개수, n_c는 순환층에 있는 셀의 개수다. \n",
    "### 입력 데이터와 w_1x를 내적하면 크기가 (m, n_c)인 행렬이 나온다. 선형 계산을 해야하니 이전 스탭의 출력값과 w_1h의 내적곱 역시 (m, n_c)인 행렬이 나와야한다. \n",
    "### H_p = (m, n_c), W_1h = (n_c, n_c)여야 (m, n_c)가 나온다. 왜 H_p가 (m, n_c)면 얘는 이전 스탭에서 순환 레이어의 출력값이었기 때문이다. 그러니 (m, n_c)가 아니면 말이 안된다. \n",
    "### 두번째 층(출력층)은 H*W_2인데 W_2는 (n_c, n_classes)다. 여기서 n_classes는 분류할 클래스 갯수인데 이진 분류인 경우엔 1이 되므로 H*W_2 = (m, n_c) * (n_c, 1) = (m, 1)이 된다.\n",
    "### 참고로 b_1 = (n_c, 1), b_2 = (n_classes, 1)이다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 순환 신경망의 역방향 계산 \n",
    "### 앞서 배운 레이어들의 역방향 계산과 같이 체인 룰을 이용한다. 뒤에서부터 계산하니 우선 두번 째 레이어에 대한 역방향 계산을 수행한다. 우선 손실함수에 대한 도함수를 구하자.\n",
    "### 그러면 d(L)/d(W_2) = d(L)/d(Z_2) * d(Z_2)/d(w_2) = H^t(-(Y-A_2)),  d(L)/d(b_2) = d(L)/d(Z_2) * d(Z_2)/d(b_2) = 1^t(-(Y-A_2)) 가 나온다.  \n",
    "### d(L)/d(W_2)에서 L은 시그모이드니까 1/(1+e^(-Z_2))고 Z_2 = H*W_2 + b_2이므로 d(L)/d(Z_2) * d(Z_2)/d(w_2)는 각 미분을 곱한 (-(Y-A_2)) * H^t = H^t(-(Y-A_2))이 된다. d(Z_2)/d(b_2)는 Z_2에서 b_2의 개수가 1이므로 미분하면 1이 되는 것은 당연하다. \n",
    "### 아무튼 그렇게 구했다. 역방향 계산이니 이제 한단계 앞으로 나가서 완전 연결 레이어의 출력값 Z_2에 대한 도함수를 구해보자. Z_2는 H에 대한 함수이므로\n",
    "### d(Z_2)/d(H) = d(H*W_2 + b_2)/d(H) = W_2가 나온다. 끝.\n",
    "### 이제 H에 대한 도함수를 구해보자. H를 구할 때 사용한 입력값은 Z_1이다. 그러니 d(H)/d(Z_1)을 구하면 된다. H는 tanh를 통해 얻는 값이다. 즉, tanh(Z_1) = H이므로 d(H)/d(Z_1) = d(tanh(Z_1))/d(Z_1)이다. tanh(x)의 미분은 1-tanh^2(x)이므로 d(H)/d(Z_1) = 1-tanh^2(Z_1) = 1-H^2다. \n",
    "### 그러면 이제 우리는 d(L)/d(Z_1) = d(L)/d(Z_2) * d(Z_2)/d(H) *  d(H)/d(Z_1)이란 식을 얻을 수 있다. 체인 룰을 이용해 첫번 째 레이어(순환 레이어) 의 출력값까지 온 것이다. \n",
    "### 이제 첫번 째 레이어의 가중치와 절편까지 가보자. \n",
    "### Z_1은 W_1x, W_1h, b_1로 이루어져있다(Z_1 = X*W_1x + H_p*W_1h + b_1). 그러니 이 3개에 대한 Z_1의 도함수를 구해야 한다. 그래야 훈련을 시킬 수 있으니까. \n",
    "### 계산은 쉬워보인다. Z_1 = X*W_1x + H_p*W_1h + b_1라 선형식으로 보이기 때문이다. 허나 여기서 유심히 봐야할게 H_p는 이전 데이터 셋에 대한 연산의 출력값이다. 즉, H_p에 W_1h가 들어있다는 뜻이며 고로 상수가 아니라는 뜻이다. 그래서 함부로 도함수를 구할 수 없다. \n",
    "### * H_p = tanh(Z_1p), Z_1p = X_p*W_1x + H_pp*W_1h + b_1(H_pp : 전전 출력값)\n",
    "### 아무튼 중요한건 Z_1의 입력값인 H_p가 변수였다는 거고 그러면 H_p * W_1h는 변수 * 변수 => 함수 * 함수로 볼 수 있다. 이를 미분하는 방법은? 고등학교 때 배운 곱셈의 미분을 이용하면 된다. \n",
    "### 그러니 d(Z_1)/d(W_1h) = d(X*W_1x + H_p*W_1h + b_1)/d(W_1h) = d(X*W_1x)/d(W_1h) + H_p*d(W_1h)/d(W_1h) + W_1h*d(H_p)/d(W_1h) + d(b_1)/d(W_1h) = H_p + W_1h*d(H_p)/d(W_1h)이 된다. \n",
    "### d(H_p)/d(W_1h)를 구해보자. d(H_p)/d(W_1h)는 H_p가 이전 데이터셋의 출력값 Z_1p에 의한 값이고 Z_1은 W_1x, W_1h, b_1으로 이뤄진 값인데 W_1h가 있으므로  d(H_p)/d(W_1h) = d(H_p)/d(Z_1p) * d(Z_1p)/ d(W_1h)로 나타낼 수 있다. \n",
    "### d(H_p)/d(Z_1p)는 d(H)/d(Z_1)을 구할 때와 같이 1-H_p^2가 나오니 d(Z_1)/d(W_1h) = H_p + W_1h*(1-H_p^2)*(d(Z_1p)/ d(W_1h))가 되며 d(Z_1p)/ d(W_1h)역시 이전 데이터 셋의 출력값 Z_1pp를 이용해 같은 과정을 반복하고...맨 처음 데이터셋까지 가야 식이 마무리된다. 존나 꼬리를 물고 물고 물고의 반복이로군\n",
    "### 이를 시간을 거를러 역전파(Backpropagation Through Time : BPTT)라고 한다. 순환 구조라 어쩔 수 없다. 그리고 식을 보면 알 수 있듯 앞서 구한 출력값들이 모두 식에 사용된다. 그래서 역대 출력값들을 저장해놓은 공간은 필수다. \n",
    "### 대충 감잡았겠지만 d(H_p)/d(Z_1p), d(H_pp)/d(Z_1pp), d(H_ppp)/d(Z_1ppp),...계속 나온다. 즉 (1-H^2)꼴의 항이 계속 나온다는 뜻이지. \n",
    "### W_1h에 대해 구했으니 이제 W_1x에 대해 구해보자. 이는 현재 입력값 X랑 곱하는 가중치다. d(Z_1)/d(W_1x) = d(X*W_1x + H_p*W_1h + b_1)/d(W_1x)인데 H_p는 W_1x에 대한 함수이기도 하다. 여기서도 함수 * 함수에 대한 미분이 적용된다는 뜻.\n",
    "###  d(X*W_1x)/d(W_1x) + H_p*d(W_1h)/d(W_1x) + W_1h*d(H_p)/d(W_1x) + d(b_1)/d(W_1x) = X + W_1h*d(H_p)/d(W_1x)가 되며 이전과 비슷하게 d(H_p)/d(W_1x) = d(H_p)/d(Z_p) * d(Z_p)/d(W_1x)로 전개가 가능하다.\n",
    "### 그럼 뭐 d(H_p)/d(Z_p) = 1-H_p^2니까 d(H_p)/d(W_1x) = 1-H_p^2 * d(Z_p)/d(W_1x)가 되고 d(Z_p)/d(W_1x)는 또 H_pp 있으니까 이전 데이터의 출력값 Z_pp 참고하고...\n",
    "### 마지막으로 d(Z_1)/d(b_1)을 구해보자. H_p는 b_1의 함수이기도 하니 역시 1-H_p^2나오는 등 같은 전개과정을 거친다. \n",
    "### 아무튼 그렇게 순환 레이어의 가중치, 절편에 대한 손실함수의 도함수 \n",
    "### d(L)/d(w_1h) =  d(L)/d(Z_2) * d(Z_2)/d(H) *  d(H)/d(Z_1) * d(Z_1)/d(W_1h)\n",
    "### d(L)/d(w_1x) =  d(L)/d(Z_2) * d(Z_2)/d(H) *  d(H)/d(Z_1) * d(Z_1)/d(W_1x)\n",
    "### d(L)/d(b_1) =  d(L)/d(Z_2) * d(Z_2)/d(H) *  d(H)/d(Z_1) * d(Z_1)/d(b_1)\n",
    "### 을 구했다! 계산은 뭐...컴퓨터가 해주겠지? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 코딩을 할 때 d(L)/d(Z_1)는 err_to_cell이라고 표현한다. 즉, 최종 그레디언트 d(L)/d(w_1h), d(L)/d(w_1x), d(L)/d(b_1)은\n",
    "### d(L)/d(w_1h) = err_to_cell * d(Z_1)/d(w_1h)\n",
    "### d(L)/d(w_1x) = err_to_cell * d(Z_1)/d(w_1x)\n",
    "### d(L)/d(b_1) = err_to_cell * d(Z_1)/d(b_1)\n",
    "### 이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}